# -*- coding: utf-8 -*-
"""TURF_ANALYSIS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OXCiH99DLLHJurEsnuxwZkHNssrBJYqX
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
from scipy.stats import skew
# Ensure the previous cell (upload) has been executed
if 'uploaded' in locals():
    # Load the uploaded Excel file
    df = pd.read_excel(list(uploaded.keys())[0])
    print("✅ File loaded successfully!")

    # 1. Number of records
    num_rows = df.shape[0]
    print(f"📄 Number of records (respondents): {num_rows}")

    # 2. Count how many message columns exist (e.g., M1_Differentiated, M2_Believable, etc.)
    message_ids = sorted(set(col.split("_")[0] for col in df.columns if col.startswith("M")))
    print(f"🧾 Number of unique messages: {len(message_ids)} → {', '.join(message_ids)}")
else:
    print("⚠️ Please run the file upload cell first.")

    # 3. Extract the score-type columns
score_types = ['Differentiated', 'Believable', 'Motivating']
message_ids = sorted(set(col.split("_")[0] for col in df.columns if col.startswith("M")))

   # 4. Prepare summary DataFrame
summary_rows = []

for msg in message_ids:
    for score_type in score_types:
        col_name = f"{msg}_{score_type}"
        if col_name in df.columns:
            scores = df[col_name].dropna()
            summary_rows.append({
                "Message": msg,
                "ScoreType": score_type,
                "Mean": round(scores.mean(), 2),
                "StdDev": round(scores.std(), 2),
                "Skew": round(skew(scores), 2)
            })

summary_df = pd.DataFrame(summary_rows)

# Collapse into plain text table for GPT
# Convert Summary to GPT Prompt
grouped_summary = summary_df.groupby("ScoreType")

prompt = "You are a message effectiveness analyst. Based on the following summary of Top-2-Box PET message scores, recommend whether to use Arithmetic Mean or Geometric Mean to combine Differentiated, Believable, and Motivating scores for each message.\n\n"

for name, group in grouped_summary:
    prompt += f"\n--- {name.upper()} ---\n"
    for _, row in group.iterrows():
        prompt += f"{row['Message']}: Mean={row['Mean']}, StdDev={row['StdDev']}, Skew={row['Skew']}\n"

prompt += "\nPlease recommend whether Arithmetic Mean or Geometric Mean is better and why, in 2-3 sentences."

import openai
client = openai.OpenAI(api_key="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")  # Replace with your key

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are an expert in pharmaceutical message testing and analytics."},
        {"role": "user", "content": prompt}
    ],
    temperature=0.1
)

print("\n🤖 GPT Recommendation:")
print(response.choices[0].message.content)

scoring_method = input("Choose scoring method (AM/GM): ").strip().upper()

from scipy.stats import gmean

print("\n📊 Calculating Message Effectiveness Scores...")

# Create a new DataFrame to store effectiveness scores
effectiveness_df = pd.DataFrame()

# Loop through message IDs
for i in range(1, int(df.shape[1]/3.0)):
    diff_col = f"M{i}_Differentiated"
    bel_col = f"M{i}_Believable"
    mot_col = f"M{i}_Motivating"

    if scoring_method == "AM":
        effectiveness_df[f"M{i}_Effectiveness"] = df[[diff_col, bel_col, mot_col]].mean(axis=1)
    elif scoring_method == "GM":
        effectiveness_df[f"M{i}_Effectiveness"] = df[[diff_col, bel_col, mot_col]].apply(
            lambda row: gmean(row[row > 0]) if all(row > 0) else 0, axis=1
        )

import matplotlib.pyplot as plt
import seaborn as sns

# Compute average effectiveness score per message
mean_effectiveness = effectiveness_df.mean().sort_values(ascending=False)

# Clean column names for display (remove "_Effectiveness")
mean_effectiveness.index = [col.split("_")[0] for col in mean_effectiveness.index]

# Plot the bar chart
plt.figure(figsize=(12, 6))
ax = sns.barplot(x=mean_effectiveness.index, y=mean_effectiveness.values, palette="Blues_d")

# Add labels on top of each bar
for i, bar in enumerate(ax.patches):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2.0, height + 0.05, f"{height:.2f}",
            ha='center', va='bottom', fontsize=10)

# Axis labels and title
plt.title("📊 Average Effectiveness Score by Message", fontsize=14)
plt.xlabel("Message", fontsize=12)
plt.ylabel("Average Score", fontsize=12)
plt.xticks(rotation=45)
plt.ylim(0, mean_effectiveness.max() + 1)  # Add space for label
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd

# Assume `effectiveness_df` is already created (M1_Effectiveness to M13_Effectiveness)
print("\n📌 Flatliner Detection and Removal Based on Variance")

# Step 1: Ask user whether to remove flatliners
choice = input("Do you want to remove flatliners? (Yes/No): ").strip().upper()

if choice != "YES":
    print("ℹ️ Flatliner removal skipped. Proceeding with full dataset.")
    effectiveness_df_cleaned = effectiveness_df.copy()
else:
    # Step 2: Ask for variance threshold
    try:
        var_threshold = float(input("Enter variance threshold (e.g., 0.1): "))
    except:
        var_threshold = 0.1
        print("⚠️ Invalid input. Using default threshold of 0.1")

    # Step 3: Compute variance across M1–M13 effectiveness for each respondent
    row_variances = effectiveness_df.var(axis=1)

    # Step 4: Identify rows below threshold
    flatliner_mask = row_variances <= var_threshold
    num_flatliners = flatliner_mask.sum()
    num_retained = len(effectiveness_df) - num_flatliners

    # Step 5: Remove flatliners
    effectiveness_df_cleaned = effectiveness_df[~flatliner_mask]

    print(f"\n🧹 Flatliner removal complete.")
    print(f"• Removed records (variance <= {var_threshold}): {num_flatliners}")
    print(f"• Retained records: {num_retained}")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.mixture import GaussianMixture
import numpy as np

# Use effectiveness_df_cleaned if flatliners were removed, else use effectiveness_df
if 'effectiveness_df_cleaned' in locals():
    input_df = effectiveness_df_cleaned.copy()
else:
    input_df = effectiveness_df.copy()

# Step 1: Ask scoring method
method_choice = input("Choose TURF input type - Type 'T2B' or 'Index' or 'Segment based Index': ").strip().upper()

if method_choice == "T2B":
    print("🔹 You selected: Top 2 Box (T2B) – converting effectiveness > 5 to 1, else 0.")
    binarized_df = input_df.applymap(lambda x: 1 if x > 5 else 0)

elif method_choice == "INDEX":
    threshold_pct = float(input("Set threshold for binarization (e.g., 5 for 5% above respondent mean): "))
    print(f"📏 Threshold set to {threshold_pct}%")

    binarized_df = input_df.copy()
    for idx, row in input_df.iterrows():
        threshold = row.mean() * (1 + threshold_pct / 100)
        binarized_df.loc[idx] = row.apply(lambda x: 1 if x >= threshold else 0)

elif method_choice == "SEGMENT BASED INDEX":
    print("🔹 You selected: Segment Based Index using GMM Clustering")
    threshold_pct = float(input("Set threshold for segment index binarization (e.g., 5 for 5% above segment mean): "))
    threshold_index = 100 + threshold_pct
    n_clusters = 4  # You can modify this if needed

    print(f"📊 Running GMM clustering with {n_clusters} segments...")
    print(f"📏 Using respondent-to-segment index threshold of {threshold_index:.1f}")

    gmm = GaussianMixture(n_components=n_clusters, random_state=42)
    segments = gmm.fit_predict(input_df)

    input_df["Segment"] = segments
    binarized_df = pd.DataFrame(0, index=input_df.index, columns=input_df.columns.drop("Segment"))

    for msg in binarized_df.columns:
        # Compute segment-level mean for the message
        segment_means = input_df.groupby("Segment")[msg].mean()

        # Compute index for each respondent
        for idx in input_df.index:
            seg = input_df.loc[idx, "Segment"]
            seg_mean = segment_means.loc[seg]

            if seg_mean == 0:
                index_score = 0
            else:
                index_score = (input_df.loc[idx, msg] / seg_mean) * 100

            if index_score > threshold_index:
                binarized_df.loc[idx, msg] = 1

    input_df.drop(columns=["Segment"], inplace=True)

else:
    print("❌ Invalid input. Re-run the cell.")
    raise SystemExit()

# Step 2: Show summary
summary_counts = binarized_df.sum().sort_values(ascending=False)
total_respondents = len(binarized_df)

print("\n📊 Message-level distribution of 1s (selected) and 0s (not selected):")
for msg in summary_counts.index:
    ones = summary_counts[msg]
    zeros = total_respondents - ones
    pct = (ones / total_respondents) * 100
    print(f"{msg}: {ones} (1s), {zeros} (0s) → {pct:.1f}% marked as 1")

# Step 3: Bar Chart
sns.set(style="whitegrid")
plt.figure(figsize=(12, 6))
sns.barplot(x=summary_counts.index, y=summary_counts.values, palette="Blues_d")
plt.xlabel("Message", fontsize=12)
plt.ylabel("No. of Respondents Marked as 1", fontsize=12)
plt.title("Message Reach Based on Binary Effectiveness", fontsize=14)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

num_rows = input_df.shape[0]
print(f"📄 Number of records (respondents): {num_rows}")

# TURF analysis

import itertools
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Simplify column names to just M1, M2, ...
turf_input_df = binarized_df.copy()
display_names = {col: col.split("_")[0] for col in turf_input_df.columns}

# Set bundle size range
min_bundle_size = 1
max_bundle_size = min(5, len(turf_input_df.columns))  # Adjust 5 as needed

# Optional: guardrails
guardrail_vars = []  # e.g., ["M1", "M6"]

# Run cumulative greedy TURF analysis
results = []
remaining = list(turf_input_df.columns)
best_combo = []

for k in range(min_bundle_size, max_bundle_size + 1):
    best_var = None
    best_reach = -1
    best_freq = -1

    for var in remaining:
        test_combo = best_combo + [var]
        test_df = turf_input_df[test_combo]
        respondent_reach = (test_df.sum(axis=1) > 0)
        reach = respondent_reach.mean()
        freq = test_df[respondent_reach].sum(axis=1).mean()

        # Guardrail enforcement (optional)
        if k == 1 and guardrail_vars:
            if var not in guardrail_vars:
                continue

        if reach > best_reach or (reach == best_reach and freq > best_freq):
            best_var = var
            best_reach = reach
            best_freq = freq

    best_combo.append(best_var)
    remaining.remove(best_var)

    results.append({
        "Messages in Bundle": k,
        "Reach (%)": round(best_reach * 100, 2),
        "Avg Frequency": round(best_freq, 2),
        "Best Combination": ", ".join([display_names[m] for m in best_combo])
    })

# Convert to DataFrame
turf_summary_df = pd.DataFrame(results)

# Display table
print("📋 Best Message Combinations and Reach by Bundle Size:")
display(turf_summary_df)

# Plot TURF curve
plt.figure(figsize=(10, 6))
sns.lineplot(data=turf_summary_df, x="Messages in Bundle", y="Reach (%)", marker="o", color="green")
plt.title("📈 TURF Reach Curve")
plt.xlabel("Number of Messages in Bundle")
plt.ylabel("Reach (%)")
plt.grid(True)
plt.tight_layout()
plt.show()

# Store for Monte Carlo
results = []
best_combos = {}  # ✅ Add this line to store best combos for Phase 5

for k in range(min_bundle_size, max_bundle_size + 1):
    combos = list(itertools.combinations(turf_input_df.columns, k))
    max_reach = -1
    max_freq = -1
    best_combo = None
    for combo in combos:
        test_df = turf_input_df[list(combo)]
        respondent_reach = (test_df.sum(axis=1) > 0)
        reach = respondent_reach.mean()
        freq = test_df[respondent_reach].sum(axis=1).mean()
        if reach > max_reach or (reach == max_reach and freq > max_freq):
            max_reach = reach
            max_freq = freq
            best_combo = combo
    results.append({
        "Messages in Bundle": k,
        "Reach (%)": round(max_reach * 100, 2),
        "Best Combination": ", ".join([display_names[m] for m in best_combo])
    })
    best_combos[k] = list(best_combo).copy()  # ✅ Save combo for Monte Carlo comparison

    # Build prompt from TURF summary
prompt = "You are a message optimization analyst. Based on the following TURF analysis (reach vs. bundle size), please recommend the most optimal number of messages to use, balancing reach and simplicity. Here is the data:\n\n"

for _, row in turf_summary_df.iterrows():
    prompt += f"{int(row['Messages in Bundle'])} messages → Reach: {row['Reach (%)']}%\n"

prompt += "\nPlease explain which bundle size to select and why (in 1–2 sentences)."

def test_last_message_alternatives(turf_input_df, best_combo_dict, bundle_size, display_names, top_n=3):
    if bundle_size not in best_combo_dict:
        print(f"❌ No TURF result found for bundle size {bundle_size}.")
        return

    current_combo = list(best_combo_dict[bundle_size])
    last_msg = current_combo[-1]
    base_combo = current_combo[:-1]

    # Reach and frequency for current combo
    reach_mask = (turf_input_df[current_combo].sum(axis=1) > 0)
    base_reach = reach_mask.mean()
    base_freq = turf_input_df[current_combo][reach_mask].sum(axis=1).mean()

    print(f"\n📋 TURF Best Combination at Size {bundle_size}: {[display_names.get(m, m) for m in current_combo]}")
    print(f"🎯 Reach: {base_reach * 100:.2f}%, Avg Frequency: {base_freq:.2f} (last message: {display_names.get(last_msg, last_msg)})")

    # Try alternatives to the last message
    candidate_pool = [col for col in turf_input_df.columns if col not in base_combo]
    alt_results = []

    for alt_msg in candidate_pool:
        test_combo = base_combo + [alt_msg]
        reach_mask = (turf_input_df[test_combo].sum(axis=1) > 0)
        reach = reach_mask.mean()
        freq = turf_input_df[test_combo][reach_mask].sum(axis=1).mean()
        alt_results.append((alt_msg, round(reach * 100, 2), round(freq, 2)))

    # Sort by reach (break ties using frequency)
    alt_results.sort(key=lambda x: (x[1], x[2]), reverse=True)

    print(f"\n💡 Top {top_n} alternatives to last message:")
    for msg, reach, freq in alt_results[:top_n]:
        label = display_names.get(msg, msg)
        boost = reach - round(base_reach * 100, 2)
        note = "🔺 Better" if boost > 0 else "➖ Same" if boost == 0 else "🔻 Worse"
        print(f"- {label} → Reach: {reach:.2f}%, Frequency: {freq:.2f} ({note}, Δ = {boost:+.2f}%)")

    # Ask user if they want to replace last message
    replace = input("\n✏️ Do you want to replace the last message? (y/n): ").strip().lower()
    if replace == "y":
        print("Enter the index (1-based) of the replacement message:")
        for i, (msg, reach, freq) in enumerate(alt_results[:top_n], start=1):
            print(f"{i}. {display_names.get(msg, msg)} → Reach: {reach:.2f}%, Freq: {freq:.2f}")
        try:
            idx = int(input("Your choice: ")) - 1
            if 0 <= idx < top_n:
                new_msg = alt_results[idx][0]
                new_combo = base_combo + [new_msg]
                best_combo_dict[bundle_size] = new_combo
                print(f"✅ Updated combo at size {bundle_size}: {[display_names.get(m, m) for m in new_combo]}")
            else:
                print("❌ Invalid index. Keeping original combo.")
        except:
            print("❌ Invalid input. Keeping original combo.")

    return alt_results[:top_n]

bundle_size_input = int(input("Enter the bundle size for testing alternatives"))

test_last_message_alternatives(
    turf_input_df=turf_input_df,
    best_combo_dict=best_combos,
    bundle_size=bundle_size_input,
    display_names=display_names
)

import openai

client = openai.OpenAI(api_key="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")  # Replace with your API key

response = client.chat.completions.create(
  model="gpt-4",
  messages=[
    {"role": "system", "content": "You are a pharma insights expert."},
    {"role": "user", "content": prompt}
  ],
  temperature=0.4
)

print("\n🤖 GPT Recommendation:")
print(response.choices[0].message.content)

import random
from collections import Counter
import itertools

print("\n📌 Phase 5: Monte Carlo Simulation – TURF Stability Testing")

# Step 1: Ask if user wants to run simulation
run_sim = input("Do you want to test the stability of the result using Monte Carlo simulation? (Yes/No): ").strip().upper()

if run_sim != "YES":
    print("❎ Simulation skipped. Process ends here.")
else:
    # Step 4: Ask for bundle size
    try:
        bundle_size = int(input("Specify number of messages in each bundle (e.g., 3): "))
    except:
        bundle_size = 3
        print("⚠️ Invalid input. Defaulting to 3.")

    # Step 6: Ask for number of iterations
    try:
        num_iterations = int(input("Specify number of Monte Carlo iterations (e.g., 25): "))
    except:
        num_iterations = 20
        print("⚠️ Invalid input. Defaulting to 20 iterations.")

    print(f"\n🔁 Running {num_iterations} iterations with bundle size {bundle_size}...")

    # Store winning combos
    winning_combos = []

    message_columns = list(turf_input_df.columns)
    n_samples = len(turf_input_df)

    for i in range(num_iterations):
        sample_df = turf_input_df.sample(frac=0.8, replace=False, random_state=i)
        combos = list(itertools.combinations(message_columns, bundle_size))

        max_reach = 0
        best_combo = None
        for combo in combos:
            reach = (sample_df[list(combo)].sum(axis=1) > 0).mean()
            if reach > max_reach:
                max_reach = reach
                best_combo = combo
        winning_combos.append(tuple(sorted(best_combo)))

    # Count frequency of each winning combo
    combo_counts = Counter(winning_combos)
    most_common = combo_counts.most_common()

    print("\n📊 Monte Carlo Summary (Top Frequent Combos):")
    for combo, freq in most_common[:5]:
        print(f"{', '.join([col.split('_')[0] for col in combo])} → {freq} wins")

    # Step 7: Compare with original recommendation from Phase 4 (if available)
    try:
        original_best = tuple(sorted(best_combos[bundle_size]))  # must match type with winning_combos
        match = "✅ YES" if original_best in combo_counts else "❌ NO"
        print(f"\n🔍 Does most frequent combo match Phase 4 recommendation? {match}")

        if match == "❌ NO":
            print("⚠️ Stability is low. Consider revisiting threshold or input method (Phase 3).")
        else:
            print("👍 Result is stable across random samples.")
    except NameError:
        print("\n⚠️ 'best_combos' from Phase 4 not found in memory.")
        print("💡 Please re-run the TURF analysis step before this phase to enable stability check.")

import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

# Count combo frequencies
combo_counts = Counter(winning_combos)
most_common = combo_counts.most_common(5)

# Prepare labels
combo_labels = [", ".join([m.split("_")[0] for m in combo]) for combo, _ in most_common]
combo_freqs = [freq for _, freq in most_common]

# Plot the frequency bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x=combo_freqs, y=combo_labels, palette="viridis")
plt.xlabel("Frequency (Wins)", fontsize=12)
plt.ylabel("Message Combination", fontsize=12)
plt.title("🔁 Top Monte Carlo Winning Message Bundles", fontsize=14)
plt.tight_layout()
plt.show()

# Generate a prompt based on Monte Carlo result
top_combo = ", ".join([m.split("_")[0] for m in most_common[0][0]])
top_freq = most_common[0][1]
prompt = f"""
Out of {num_iterations} Monte Carlo simulations, the most stable message combination was: {top_combo},
winning in {top_freq} iterations. Please summarize the implications of this result in 2-3 sentences
for a commercial insights manager.
"""

# NEW OpenAI API usage for SDK >= 1.0.0
import openai
client = openai.OpenAI(api_key="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")  # ← Replace with your actual API key

response = client.chat.completions.create(
  model="gpt-4",
  messages=[
    {"role": "system", "content": "You are a pharma insights assistant skilled in message testing."},
    {"role": "user", "content": prompt}
  ],
  temperature=0.4
)

# Print GPT summary
print("\n📝 GPT Summary:")
print(response.choices[0].message.content)

import importlib.util
import subprocess
import sys

def ensure_fpdf_installed():
    if importlib.util.find_spec("fpdf") is None:
        subprocess.run(
            [sys.executable, "-m", "pip", "install", "fpdf"],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )

# Call the function
ensure_fpdf_installed()

from collections import Counter

# Step 1: Prepare simulation result summary
combo_counts = Counter(winning_combos)
most_common = combo_counts.most_common(5)

# Step 2: Format as readable string
summary_text = "🔁 Top Monte Carlo Winning Combinations:\n\n"
for combo, freq in most_common:
    readable_combo = ", ".join([m.split("_")[0] for m in combo])
    summary_text += f"{readable_combo}: {freq} wins\n"

# Step 3: Add GPT summary
summary_text += "\n📝 GPT Summary:\n"
summary_text += response.choices[0].message.content

with open("monte_carlo_summary.txt", "w") as f:
    f.write(summary_text)

import re

def remove_non_ascii(text):
    return re.sub(r'[^\x00-\x7F]+', '', text)  # removes all non-ASCII characters
clean_text = remove_non_ascii(summary_text)

from fpdf import FPDF

pdf = FPDF()
pdf.add_page()
pdf.set_font("Arial", size=12)

# Write clean text line by line
for line in clean_text.splitlines():
    pdf.multi_cell(0, 10, line)

# Save the PDF
pdf.output("Monte_Carlo_Stability_Report.pdf")

from google.colab import files
files.download("Monte_Carlo_Stability_Report.pdf")